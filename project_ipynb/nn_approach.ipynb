{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS is available and set the device accordingly\n",
    "device = 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>artists</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>popularity</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>year</th>\n",
       "      <th>Song</th>\n",
       "      <th>Weeks on Chart</th>\n",
       "      <th>Average Previous Week Position</th>\n",
       "      <th>Week Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8050</td>\n",
       "      <td>gene autry</td>\n",
       "      <td>0.838</td>\n",
       "      <td>150160</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3010</td>\n",
       "      <td>-12.850</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>96.638</td>\n",
       "      <td>0.976</td>\n",
       "      <td>1947</td>\n",
       "      <td>here comes santa claus right down santa claus ...</td>\n",
       "      <td>10</td>\n",
       "      <td>56.0</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9450</td>\n",
       "      <td>ray price</td>\n",
       "      <td>0.308</td>\n",
       "      <td>184933</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1790</td>\n",
       "      <td>-10.064</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>67.086</td>\n",
       "      <td>0.214</td>\n",
       "      <td>1956</td>\n",
       "      <td>danny boy</td>\n",
       "      <td>8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7660</td>\n",
       "      <td>johnny mathis</td>\n",
       "      <td>0.249</td>\n",
       "      <td>177893</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>-10.163</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>90.207</td>\n",
       "      <td>0.171</td>\n",
       "      <td>1959</td>\n",
       "      <td>someone</td>\n",
       "      <td>13</td>\n",
       "      <td>45.0</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8730</td>\n",
       "      <td>conway twitty</td>\n",
       "      <td>0.467</td>\n",
       "      <td>146893</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2530</td>\n",
       "      <td>-8.596</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>83.814</td>\n",
       "      <td>0.880</td>\n",
       "      <td>1959</td>\n",
       "      <td>mona lisa</td>\n",
       "      <td>12</td>\n",
       "      <td>47.0</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8380</td>\n",
       "      <td>jimmy reed</td>\n",
       "      <td>0.602</td>\n",
       "      <td>144442</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>-7.576</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>111.646</td>\n",
       "      <td>0.558</td>\n",
       "      <td>1959</td>\n",
       "      <td>down in virginia</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13654</th>\n",
       "      <td>0.2300</td>\n",
       "      <td>breland</td>\n",
       "      <td>0.883</td>\n",
       "      <td>158329</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>-8.382</td>\n",
       "      <td>68</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>136.982</td>\n",
       "      <td>0.550</td>\n",
       "      <td>2020</td>\n",
       "      <td>my truck</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13655</th>\n",
       "      <td>0.7600</td>\n",
       "      <td>giveon</td>\n",
       "      <td>0.640</td>\n",
       "      <td>260776</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>-7.757</td>\n",
       "      <td>68</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>119.513</td>\n",
       "      <td>0.437</td>\n",
       "      <td>2020</td>\n",
       "      <td>like i want you</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13656</th>\n",
       "      <td>0.1060</td>\n",
       "      <td>sean paul</td>\n",
       "      <td>0.951</td>\n",
       "      <td>218573</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0712</td>\n",
       "      <td>-4.675</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>125.040</td>\n",
       "      <td>0.822</td>\n",
       "      <td>2020</td>\n",
       "      <td>temperature</td>\n",
       "      <td>31</td>\n",
       "      <td>85.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13657</th>\n",
       "      <td>0.0236</td>\n",
       "      <td>lil uzi vert</td>\n",
       "      <td>0.775</td>\n",
       "      <td>234627</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>-5.353</td>\n",
       "      <td>66</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>155.086</td>\n",
       "      <td>0.490</td>\n",
       "      <td>2020</td>\n",
       "      <td>p2</td>\n",
       "      <td>6</td>\n",
       "      <td>43.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13658</th>\n",
       "      <td>0.1090</td>\n",
       "      <td>ingrid andress</td>\n",
       "      <td>0.512</td>\n",
       "      <td>214787</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>-7.387</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>80.588</td>\n",
       "      <td>0.366</td>\n",
       "      <td>2020</td>\n",
       "      <td>more hearts than mine</td>\n",
       "      <td>20</td>\n",
       "      <td>36.0</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13659 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       acousticness         artists  danceability  duration_ms  energy  \\\n",
       "0            0.8050      gene autry         0.838       150160   0.354   \n",
       "1            0.9450       ray price         0.308       184933   0.302   \n",
       "2            0.7660   johnny mathis         0.249       177893   0.317   \n",
       "3            0.8730   conway twitty         0.467       146893   0.626   \n",
       "4            0.8380      jimmy reed         0.602       144442   0.796   \n",
       "...             ...             ...           ...          ...     ...   \n",
       "13654        0.2300         breland         0.883       158329   0.299   \n",
       "13655        0.7600          giveon         0.640       260776   0.355   \n",
       "13656        0.1060       sean paul         0.951       218573   0.600   \n",
       "13657        0.0236    lil uzi vert         0.775       234627   0.720   \n",
       "13658        0.1090  ingrid andress         0.512       214787   0.428   \n",
       "\n",
       "       instrumentalness  key  liveness  loudness  popularity  speechiness  \\\n",
       "0              0.000000    1    0.3010   -12.850           0       0.0369   \n",
       "1              0.023800    3    0.1790   -10.064           7       0.0290   \n",
       "2              0.000001    9    0.2920   -10.163          15       0.0309   \n",
       "3              0.000033    4    0.2530    -8.596          21       0.0465   \n",
       "4              0.002550    9    0.0784    -7.576          11       0.0675   \n",
       "...                 ...  ...       ...       ...         ...          ...   \n",
       "13654          0.000008    2    0.1620    -8.382          68       0.1050   \n",
       "13655          0.000070   10    0.1140    -7.757          68       0.0650   \n",
       "13656          0.000000    0    0.0712    -4.675           1       0.0686   \n",
       "13657          0.000000   11    0.1140    -5.353          66       0.1930   \n",
       "13658          0.000000    0    0.1050    -7.387          65       0.0271   \n",
       "\n",
       "         tempo  valence  year  \\\n",
       "0       96.638    0.976  1947   \n",
       "1       67.086    0.214  1956   \n",
       "2       90.207    0.171  1959   \n",
       "3       83.814    0.880  1959   \n",
       "4      111.646    0.558  1959   \n",
       "...        ...      ...   ...   \n",
       "13654  136.982    0.550  2020   \n",
       "13655  119.513    0.437  2020   \n",
       "13656  125.040    0.822  2020   \n",
       "13657  155.086    0.490  2020   \n",
       "13658   80.588    0.366  2020   \n",
       "\n",
       "                                                    Song  Weeks on Chart  \\\n",
       "0      here comes santa claus right down santa claus ...              10   \n",
       "1                                              danny boy               8   \n",
       "2                                                someone              13   \n",
       "3                                              mona lisa              12   \n",
       "4                                       down in virginia               2   \n",
       "...                                                  ...             ...   \n",
       "13654                                           my truck               2   \n",
       "13655                                    like i want you               3   \n",
       "13656                                        temperature              31   \n",
       "13657                                                 p2               6   \n",
       "13658                              more hearts than mine              20   \n",
       "\n",
       "       Average Previous Week Position  Week Position  \n",
       "0                                56.0          135.0  \n",
       "1                                33.0           73.0  \n",
       "2                                45.0          118.0  \n",
       "3                                47.0          129.0  \n",
       "4                                 4.0           13.0  \n",
       "...                               ...            ...  \n",
       "13654                             8.0            5.0  \n",
       "13655                             2.0            4.0  \n",
       "13656                            85.0          180.0  \n",
       "13657                            43.0          115.0  \n",
       "13658                            36.0          127.0  \n",
       "\n",
       "[13659 rows x 18 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "# data = {\n",
    "#     \"acousticness\": [0.8050, 0.9450, 0.7660, 0.8730, 0.8380],\n",
    "#     \"year\": [1947, 1956, 1959, 1959, 1959],\n",
    "#     \"Weeks on Chart\": [10, 8, 13, 12, 2],\n",
    "#     \"loudness\": [-12.850, -10.064, -10.163, -8.596, -7.576],\n",
    "#     \"energy\": [0.354, 0.302, 0.317, 0.626, 0.796],\n",
    "#     \"speechiness\": [0.0369, 0.0290, 0.0309, 0.0465, 0.0675],\n",
    "#     \"danceability\": [0.838, 0.308, 0.249, 0.467, 0.602],\n",
    "#     \"Average Previous Week Position\": [56.0, 33.0, 45.0, 47.0, 4.0],\n",
    "#     \"Week Position\": [135.0, 73.0, 118.0, 129.0, 13.0],\n",
    "#     \"duration_ms\": [150160, 184933, 177893, 146893, 144442],\n",
    "#     \"tempo\": [96.638, 67.086, 90.207, 83.814, 111.646],\n",
    "#     \"key\": [1, 3, 9, 4, 9],\n",
    "#     \"liveness\": [0.3010, 0.1790, 0.2920, 0.2530, 0.0784],\n",
    "#     \"instrumentalness\": [0.000000, 0.023800, 0.000001, 0.000033, 0.002550],\n",
    "#     \"valence\": [0.976, 0.214, 0.171, 0.880, 0.558],\n",
    "#     \"acousticness\": [0.8050, 0.9450, 0.7660, 0.8730, 0.8380],\n",
    "#     \"popularity\": [0, 7, 15, 21, 11]\n",
    "# }\n",
    "\n",
    "df = pd.read_csv('result.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target\n",
    "features = [\"year\", \"Weeks on Chart\", \"loudness\", \"energy\", \"speechiness\", \"danceability\", \"Average Previous Week Position\", \"Week Position\", \"duration_ms\", \"tempo\", \"key\", \"liveness\", \"instrumentalness\", \"valence\", \"acousticness\"]\n",
    "X = df[features].values\n",
    "y = df[\"popularity\"].values\n",
    "\n",
    "# Split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(features), 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Training Loss: 103.5156, Test Loss: 144.9700\n",
      "Epoch [20/1000], Training Loss: 104.1945, Test Loss: 146.5417\n",
      "Epoch [30/1000], Training Loss: 104.2439, Test Loss: 145.9766\n",
      "Epoch [40/1000], Training Loss: 104.4535, Test Loss: 146.3252\n",
      "Epoch [50/1000], Training Loss: 103.2500, Test Loss: 146.1093\n",
      "Epoch [60/1000], Training Loss: 104.3159, Test Loss: 146.8744\n",
      "Epoch [70/1000], Training Loss: 104.5884, Test Loss: 146.7084\n",
      "Epoch [80/1000], Training Loss: 102.0526, Test Loss: 146.8077\n",
      "Epoch [90/1000], Training Loss: 104.1805, Test Loss: 146.5468\n",
      "Epoch [100/1000], Training Loss: 101.1379, Test Loss: 146.6470\n",
      "Epoch [110/1000], Training Loss: 102.4756, Test Loss: 146.8343\n",
      "Epoch [120/1000], Training Loss: 104.1645, Test Loss: 146.8743\n",
      "Epoch [130/1000], Training Loss: 102.3819, Test Loss: 146.5654\n",
      "Epoch [140/1000], Training Loss: 101.3460, Test Loss: 146.5269\n",
      "Epoch [150/1000], Training Loss: 102.5660, Test Loss: 147.1671\n",
      "Epoch [160/1000], Training Loss: 105.0394, Test Loss: 146.4534\n",
      "Epoch [170/1000], Training Loss: 102.5413, Test Loss: 146.8011\n",
      "Epoch [180/1000], Training Loss: 101.6834, Test Loss: 147.9132\n",
      "Epoch [190/1000], Training Loss: 101.7253, Test Loss: 147.6136\n",
      "Epoch [200/1000], Training Loss: 103.5543, Test Loss: 147.1351\n",
      "Epoch [210/1000], Training Loss: 102.6129, Test Loss: 147.0388\n",
      "Epoch [220/1000], Training Loss: 104.6454, Test Loss: 147.2690\n",
      "Epoch [230/1000], Training Loss: 101.5653, Test Loss: 147.8803\n",
      "Epoch [240/1000], Training Loss: 102.5214, Test Loss: 148.4529\n",
      "Epoch [250/1000], Training Loss: 101.9283, Test Loss: 147.5317\n",
      "Epoch [260/1000], Training Loss: 104.3737, Test Loss: 147.3868\n",
      "Epoch [270/1000], Training Loss: 102.2527, Test Loss: 147.7343\n",
      "Epoch [280/1000], Training Loss: 102.9156, Test Loss: 147.3982\n",
      "Epoch [290/1000], Training Loss: 104.1585, Test Loss: 148.1519\n",
      "Epoch [300/1000], Training Loss: 101.2941, Test Loss: 147.9808\n",
      "Epoch [310/1000], Training Loss: 102.2402, Test Loss: 148.4428\n",
      "Epoch [320/1000], Training Loss: 104.6353, Test Loss: 147.5756\n",
      "Epoch [330/1000], Training Loss: 104.0511, Test Loss: 147.7303\n",
      "Epoch [340/1000], Training Loss: 102.9966, Test Loss: 147.7164\n",
      "Epoch [350/1000], Training Loss: 101.9813, Test Loss: 148.2442\n",
      "Epoch [360/1000], Training Loss: 101.5539, Test Loss: 147.4899\n",
      "Epoch [370/1000], Training Loss: 101.7258, Test Loss: 148.3519\n",
      "Epoch [380/1000], Training Loss: 103.2521, Test Loss: 148.0836\n",
      "Epoch [390/1000], Training Loss: 102.7605, Test Loss: 147.8242\n",
      "Epoch [400/1000], Training Loss: 102.9369, Test Loss: 146.7617\n",
      "Epoch [410/1000], Training Loss: 101.4812, Test Loss: 147.8913\n",
      "Epoch [420/1000], Training Loss: 101.7465, Test Loss: 148.6182\n",
      "Epoch [430/1000], Training Loss: 100.3399, Test Loss: 147.8221\n",
      "Epoch [440/1000], Training Loss: 103.0398, Test Loss: 148.6193\n",
      "Epoch [450/1000], Training Loss: 101.4032, Test Loss: 147.6042\n",
      "Epoch [460/1000], Training Loss: 101.3565, Test Loss: 148.5831\n",
      "Epoch [470/1000], Training Loss: 100.5472, Test Loss: 148.5592\n",
      "Epoch [480/1000], Training Loss: 100.6377, Test Loss: 148.7331\n",
      "Epoch [490/1000], Training Loss: 102.1109, Test Loss: 148.0295\n",
      "Epoch [500/1000], Training Loss: 100.7330, Test Loss: 147.7390\n",
      "Epoch [510/1000], Training Loss: 100.3866, Test Loss: 147.3433\n",
      "Epoch [520/1000], Training Loss: 100.6517, Test Loss: 147.9146\n",
      "Epoch [530/1000], Training Loss: 103.0349, Test Loss: 149.1483\n",
      "Epoch [540/1000], Training Loss: 101.1260, Test Loss: 147.8156\n",
      "Epoch [550/1000], Training Loss: 104.3595, Test Loss: 149.3470\n",
      "Epoch [560/1000], Training Loss: 99.9780, Test Loss: 149.3471\n",
      "Epoch [570/1000], Training Loss: 101.6750, Test Loss: 148.3745\n",
      "Epoch [580/1000], Training Loss: 101.9462, Test Loss: 148.8047\n",
      "Epoch [590/1000], Training Loss: 100.0124, Test Loss: 148.4481\n",
      "Epoch [600/1000], Training Loss: 102.2612, Test Loss: 148.6381\n",
      "Epoch [610/1000], Training Loss: 100.5246, Test Loss: 148.7208\n",
      "Epoch [620/1000], Training Loss: 101.5643, Test Loss: 149.3696\n",
      "Epoch [630/1000], Training Loss: 101.9377, Test Loss: 148.9426\n",
      "Epoch [640/1000], Training Loss: 102.7070, Test Loss: 148.5025\n",
      "Epoch [650/1000], Training Loss: 101.3542, Test Loss: 149.0857\n",
      "Epoch [660/1000], Training Loss: 101.1653, Test Loss: 148.5492\n",
      "Epoch [670/1000], Training Loss: 105.0990, Test Loss: 150.0981\n",
      "Epoch [680/1000], Training Loss: 103.8204, Test Loss: 149.5320\n",
      "Epoch [690/1000], Training Loss: 103.4819, Test Loss: 147.9580\n",
      "Epoch [700/1000], Training Loss: 100.8763, Test Loss: 148.3751\n",
      "Epoch [710/1000], Training Loss: 101.8085, Test Loss: 148.7532\n",
      "Epoch [720/1000], Training Loss: 101.0463, Test Loss: 149.3958\n",
      "Epoch [730/1000], Training Loss: 100.8738, Test Loss: 149.5880\n",
      "Epoch [740/1000], Training Loss: 101.1153, Test Loss: 148.6719\n",
      "Epoch [750/1000], Training Loss: 102.3237, Test Loss: 148.6609\n",
      "Epoch [760/1000], Training Loss: 103.7023, Test Loss: 150.0984\n",
      "Epoch [770/1000], Training Loss: 101.4628, Test Loss: 149.9003\n",
      "Epoch [780/1000], Training Loss: 102.9465, Test Loss: 149.2625\n",
      "Epoch [790/1000], Training Loss: 102.7461, Test Loss: 148.6412\n",
      "Epoch [800/1000], Training Loss: 101.0387, Test Loss: 148.7605\n",
      "Epoch [810/1000], Training Loss: 101.4879, Test Loss: 148.3271\n",
      "Epoch [820/1000], Training Loss: 102.7567, Test Loss: 149.2270\n",
      "Epoch [830/1000], Training Loss: 103.6066, Test Loss: 148.2085\n",
      "Epoch [840/1000], Training Loss: 100.9665, Test Loss: 148.5648\n",
      "Epoch [850/1000], Training Loss: 101.8732, Test Loss: 148.6888\n",
      "Epoch [860/1000], Training Loss: 101.2790, Test Loss: 149.6770\n",
      "Epoch [870/1000], Training Loss: 101.6839, Test Loss: 149.6424\n",
      "Epoch [880/1000], Training Loss: 103.7691, Test Loss: 147.8618\n",
      "Epoch [890/1000], Training Loss: 101.0273, Test Loss: 148.6423\n",
      "Epoch [900/1000], Training Loss: 102.5915, Test Loss: 147.9165\n",
      "Epoch [910/1000], Training Loss: 101.7402, Test Loss: 148.7633\n",
      "Epoch [920/1000], Training Loss: 102.7499, Test Loss: 148.5500\n",
      "Epoch [930/1000], Training Loss: 100.6764, Test Loss: 148.5805\n",
      "Epoch [940/1000], Training Loss: 101.7872, Test Loss: 148.4354\n",
      "Epoch [950/1000], Training Loss: 101.6190, Test Loss: 149.3723\n",
      "Epoch [960/1000], Training Loss: 104.1287, Test Loss: 149.0445\n",
      "Epoch [970/1000], Training Loss: 101.0088, Test Loss: 148.4973\n",
      "Epoch [980/1000], Training Loss: 100.4681, Test Loss: 149.2583\n",
      "Epoch [990/1000], Training Loss: 99.6626, Test Loss: 149.1591\n",
      "Epoch [1000/1000], Training Loss: 100.8487, Test Loss: 148.9922\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors and move to device\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training loop with evaluation\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test_tensor)\n",
    "            test_loss = criterion(test_outputs, y_test_tensor)\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                  f'Training Loss: {loss.item():.4f}, '\n",
    "                  f'Test Loss: {test_loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggbaker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
